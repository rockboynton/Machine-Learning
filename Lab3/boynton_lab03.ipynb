{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.1"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "Rock Boynton | CS 4850 \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will implement and apply the K-Means clustering algorithm to some data, specifically the `gaussguess.csv` dataset.\n",
    "\n",
    "We will then run the following experiments:\n",
    "\n",
    "1. Initialization\n",
    "\n",
    "2. Local Optima\n",
    "\n",
    "3. Selecting K\n",
    "\n",
    "## Summary of Results\n",
    "\n",
    "TODO\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiments\n",
    "\n",
    "Before we begin our experiments, let's import some boilerplate libraries that we will need as well as define a function for plotting the datpoints with memberships colored."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_with_memberships(assignments, k):\n",
    "    for group in range(k):\n",
    "        cluster = data[assignments == group]\n",
    "        plt.scatter(cluster[:, 0], cluster[:, 1], label=group, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_intra_group_variation(trials, scores):\n",
    "    plt.scatter(range(trials), scores)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Intra-group variation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment 1: Initialization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import our KMeans class\n",
    "from kmeans import KMeans\n",
    "\n",
    "# Step 2: Load in the provided data file “gaussguess.csv” and create a 2D Numpy array from it.\n",
    "datapath = 'gaussguess.csv'\n",
    "data = np.loadtxt(datapath, delimiter=',')\n",
    "\n",
    "# Step 3: Run model on the given dataset with parameters `k = 3` and `iterations = 10` and plot first two dimensions\n",
    "k = 3\n",
    "iterations = 10\n",
    "model = KMeans(k, iterations)\n",
    "assignments = model.fit_predict(data)\n",
    "\n",
    "plot_data_with_memberships(assignments, model.k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment 2: Local Optima"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Run KMeans 5 times using `k = 3` and `iterations = 10`, storing the intra-group variation (score) in a NumPy vector. Then plot the datapoints with group membership colored. \n",
    "trials = 5\n",
    "scores = np.zeros(5)\n",
    "for trial in range(trials):\n",
    "    model = KMeans(k, iterations)\n",
    "    assignments = model.fit_predict(data)\n",
    "    scores[trial] = model.score(data)\n",
    "    plot_data_with_memberships(assignments, model.k)\n",
    "\n",
    "# Step 2: Plot each intra-group variation from each evaluation. \n",
    "plot_intra_group_variation(trials, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "As we can see, intra-group variation is not the same between variations. I don't think we can expect it to be either because the first step of the K-Means algorithm is random selection of initial cluster centers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment 3: Selecting K"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Run KMeans 10 times storing the intra-group variation in a NumPy vector each time and plot the datapoints with group membership colored. \n",
    "trials = 10\n",
    "k = 10\n",
    "scores = np.zeros(10)\n",
    "for trial, trial_k in zip(range(trials), range(1, k+1)):\n",
    "    model = KMeans(trial_k, iterations)\n",
    "    assignments = model.fit_predict(data)\n",
    "    scores[trial] = model.score(data)\n",
    "    plot_data_with_memberships(assignments, model.k)\n",
    "\n",
    "# Step 2: Plot k vs the within-cluster variance calculated by the score() method. \n",
    "plot_intra_group_variation(trials, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "As we can see, as we increase k, the intragroup variation generally decreases, however with diminishing returns. At `k = 2`/`k = 3`, the intragroup variation relatively levels out. This point is referred to as the \"elbow\", i.e. where adding more clusters doesn't give much better modeling of the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Questions\n",
    "\n",
    "a. Based on the plot of k vs the within-cluster variance, what which value of k do you think\n",
    "is most appropriate?\n",
    "\n",
    "* I think the value of `k` most appropriate for the model is 2 based on the plot of k vs the within-cluster variance, because it is right about at the \"elbow\" point where the 2 clusters explain a lot of the variance that was there with lower values of k, but any more than 2 would seem to be marginal reduction of intra-group variation. \n",
    "\n",
    "b. Based on your scatter plots, what value of k do you think is most appropriate? Did you\n",
    "come to the same answer as you did for (a)?\n",
    "\n",
    "* Intuitively looking at the datapoint scatter plots show that `k = 3` is likely a good choice because there looks to be 3 groupings. This is not the same as (a) where it looked like `k = 2` would be a good choice. `k = 3` is slightly past the elbow.\n",
    "\n",
    "c. Scikit-Learn's KMeans implementation runs KMeans 10 times by default and picks the\n",
    "best clustering according to inertia. Why do you think it might do this?\n",
    "\n",
    "* Running KMeans 10 times and picking the best clustering according to inertia seems like a decent method to get a good value of K, without being able to see the data being plotted, avoiding bad groupings from bad initial centroids. I don't think it would be neccassary to have more than 10 for most datasets."
   ]
  }
 ]
}