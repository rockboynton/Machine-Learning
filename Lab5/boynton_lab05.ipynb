{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":[" # Optimization Methods\n","\n"," Rock Boynton | CS 4850\n","\n"," ## Introduction\n","\n"," In this notebook, we will practice writing cost functions for optimization problems using an iterative optimization algorithm we developed.\n"," We will also find and distinguish between global and local optima, and then play with the initial conditions to see how they affect the result.\n","\n"," We will then run experiments on the following models:\n","\n"," 1. Cubic Model\n","\n"," 2. Quartic Model\n","\n"," 3. Gaussian Model\n","\n"," ## Summary of Results\n","\n","Experiment 1 showed us how our algorithm can work as expected to find the minimum of a function, but also showed up how much the starting parameters matter. If you choose poorly you could end up chasing infinity along a gradient, never finding an optima. Experiment 3 again showed us how the starting point matters, but this time in finding *global* vs *local* optima. Experiment 3 showed us how, more often than not, we are trying to optimize hyper-parameters of a model, and using something like mean-squared-error instead of a function to model data itself. That was tricky to determine, though.\n","\n"," ---"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import optim\n","\n","import numpy as np \n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":[" ## Experiment 1: Cubic Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CubicCostFunction:\n","    def cost(self, params):\n","        \"\"\"\n","        Implements the curve:\n","        \n","        y = x**3 - 3*x**2 - 144*x + 432\n","        \"\"\"\n","        return params**3 - 3*params**2 - 144*params + 432\n","\n","NUMERIC_DIFF_DELTA = 1e-5\n","GRADIENT_TOL = 1e-2\n","STEP_SIZE = 0.01\n","MAX_ITER = 100\n","TOL = 1e-5\n","\n","optimizer = optim.Optimizer(STEP_SIZE, MAX_ITER, GRADIENT_TOL, NUMERIC_DIFF_DELTA)\n","cost = CubicCostFunction()\n","\n","x_wide = np.arange(-30, 30, 0.01)\n","x_narrow = np.arange(-8, 15, 0.01)\n","\n","for x_range in (x_wide, x_narrow):\n","    gradient = np.array([optimizer._gradient(cost, np.array([x])) for x in x_range])\n","    plt.plot(x_range, cost.cost(x_range), x_range, gradient)\n","    for starting_params in (25, 0, -25):\n","        optimized_params, iters = optimizer.optimize(cost, np.array([starting_params], dtype=np.float64))\n","        print(f'Found min at {optimized_params} starting at {starting_params} in {iters} iterations of optimization algorithm.')\n","        params = np.array([starting_params, optimized_params])\n","        plt.plot(params, cost.cost(params), 'ro-', scalex=False, scaley=False)\n","    plt.legend(['cost function', 'gradient'])\n","    plt.show()\n"," \n","# Notice that x is at a local optima where the derivitive == 0"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":[" ## Experiment 2: Quartic Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class QuarticCostFunction:\n","    def cost(self, params):\n","        \"\"\"\n","        Implements the curve:\n","        \n","        y = 3*x**4 - 16*x**3 - 18*x**2\n","        \"\"\"\n","        return 3*params**4 - 16*params**3 - 18*params**2\n","\n","MAX_ITER = 1000\n","STEP_SIZE = 0.001\n","GRADIENT_TOL = 1e-5\n","NUMERIC_DIFF_DELTA = 1e-5\n","\n","optimizer = optim.Optimizer(STEP_SIZE, MAX_ITER, GRADIENT_TOL, NUMERIC_DIFF_DELTA)\n","cost = QuarticCostFunction()\n","\n","x_range = np.arange(-3, 6.75, 0.01)\n","\n","# plt.plot(x_range, cost.cost(x_range), x_range, optimizer._gradient(cost, x_range)) # doesn't work\n","gradient = np.array([optimizer._gradient(cost, np.array([x])) for x in x_range])\n","plt.plot(x_range, cost.cost(x_range), x_range, gradient)\n","for starting_params in (6, 2, -2):\n","    optimized_params, iters = optimizer.optimize(cost, np.array([starting_params], dtype=np.float64))\n","    print(f'Found min at {optimized_params} starting at {starting_params} in {iters} iterations of optimization algorithm.')\n","    params = np.array([starting_params, optimized_params])\n","    plt.plot(params, cost.cost(params), 'ro-', scalex=False, scaley=False)\n","plt.legend(['cost function', 'gradient'])\n","plt.show()\n"," \n","# Notice that x is at a local optima where the derivitive == 0"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":[" ## Experiment 3: Gaussian Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from math import pi, e, sqrt\n","\n","class GaussianCostFunction:\n","    def __init__(self, data):\n","        self.data = data\n","\n","\n","    def _calculate_y(self, x, mu, sigma):\n","        \"\"\"\n","        Implements the Gaussian function\n","        \"\"\"\n","        return (1 / (sigma*sqrt(2*pi))) * e**(-0.5 * ((x - mu) / sigma)**2)\n","\n","\n","    def cost(self, params):\n","        \"\"\"\n","        Implements the mean squared error of the gaussian function\n","        params[0] = mu\n","        params[1] = sigma\n","        \"\"\"\n","        return np.mean((self.data[:, 1] - self._calculate_y(self.data[:, 0], params[0], params[1])) ** 2)\n","\n","MAX_ITER = 1000\n","STEP_SIZE = 1\n","GRADIENT_TOL = 1e-5\n","NUMERIC_DIFF_DELTA = 1e-5\n","\n","datapath = \"gaussdist.csv\"\n","data = np.loadtxt(datapath, delimiter=',')\n","data = data[:, [1, 0]] # flip columns \n","\n","cost = GaussianCostFunction(data)\n","\n","x_range = np.arange(-10, 10, 0.01)\n","\n","starting_params = [1, 0.75] # mu, sigma\n","plt.plot(x_range, cost._calculate_y(x_range, starting_params[0], starting_params[1]), color='red')\n","plt.scatter(data[:, 0], data[:, 1])\n","plt.legend(['unoptimized model', 'data'])\n","plt.show()\n","\n","optimizer = optim.Optimizer(STEP_SIZE, MAX_ITER, GRADIENT_TOL, NUMERIC_DIFF_DELTA)\n","optimized_params, iters = optimizer.optimize(cost, np.array(starting_params, dtype=np.float64))\n","\n","print(f'Found min at {optimized_params} starting at {starting_params} in {iters} iterations of optimization algorithm.')\n","\n","plt.plot(x_range, cost._calculate_y(x_range, optimized_params[0], optimized_params[1]), color='red')\n","plt.scatter(data[:, 0], data[:, 1])\n","plt.legend(['optimized model', 'data'])\n","plt.show()"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":[" ## Questions\n","\n"," 1. For experiment 1, what were the solutions for steps 5, 6, and 7? Describe any\n"," differences that you saw. How do you explain these differences?\n","    - Found min at [ 8.01324051] starting at 25 in 9 iterations of optimization algorithm.\n","    - Found min at [ 7.99087673] starting at 0 in 15 iterations of optimization algorithm.\n","    - Found min at [ -3.60287971e+19] starting at -25 in 8 iterations of optimization algorithm.\n","    - Starting at 25 and 0 it makes sense that the algorithm would \"follow the gradient\" and find the min at around 8, but when starting at -25, it follows the gradient down to infinity hence the huge negative value for the calculated min.\n","\n"," 2. For experiment 2, what were the solutions for steps 5, 6, and 7? Describe any\n"," differences that you saw. Relate your knowledge of global and local optima to this\n"," discussion. What is the correct solution and why?\n","   - Found min at [ 4.64576556] starting at 6 in 30 iterations of optimization algorithm.\n","   - Found min at [ 4.64572556] starting at 2 in 41 iterations of optimization algorithm.\n","   - Found min at [-0.64598601] starting at -2 in 176 iterations of optimization algorithm.\n","   - When starting at 6 and 2, the gradient points toward the global min of about 4.646, but when starting at -2, the gradient points toward the local min of about -0.646. The correct solution is the global min, because that is where the function is truly minimized\n","\n"," 2. For experiment 3, what do you observe when comparing your model with fit parameters\n"," to your data. If you were to get a new set of independent variables, how could you use\n"," this model and what information would it give you?\n","  - When comparing the model with with fit parameters (mu = 2.9997177, sigma = 0.99997786), I see that the model lines up nicely with the curve of the dataset. If you got a new set of independant data, you could create a new cost object with the data then use the optimized mu and sigma to plot it with a scatter of the data to see how well the mean and standard deviation fit the new data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.1-final"},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"kernelspec":{"name":"python36164bitrootconda6e0c314544c443b5938248d6fa4e1245","display_name":"Python 3.6.1 64-bit ('root': conda)"}}}